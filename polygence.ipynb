{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2582a9-098d-424d-9603-fcd8f813909d",
   "metadata": {},
   "source": [
    "## EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3946cd-06a8-4434-a04d-d67a72f41293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub transformers av tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89984ab2-a64a-4191-bb48-9a619b03a751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'av'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mav\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'av'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import av\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import VivitConfig, VivitModel, VivitImageProcessor\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    # Decodes the video; container represents the video; indices are a list of frame indices to decode; returns numpy array\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    # Generates set of frame indices\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "def get_video_frame_count(container):\n",
    "    # Count the number of frames in the video\n",
    "    frame_count = 0\n",
    "    for frame in container.decode(video=0):\n",
    "        frame_count += 1\n",
    "    return frame_count\n",
    "\n",
    "# Load the model and the processor\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "# Path to the folder containing the videos\n",
    "video_folder_path = \"./Videos/\"\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "# Loop through each video file in the folder\n",
    "for i, video_file in tqdm(enumerate(os.listdir(video_folder_path))):\n",
    "    # Check if the file is a video file (e.g., .mov, .mp4)\n",
    "    if video_file.endswith(('.mov', '.mp4')):\n",
    "        video_path = os.path.join('./Videos', video_file)\n",
    "\n",
    "        try:\n",
    "            # Load the video\n",
    "            container = av.open(video_path)\n",
    "            frame_count = get_video_frame_count(container)\n",
    "\n",
    "            # Sample 32 frames\n",
    "            indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=frame_count)\n",
    "            video_frames = read_video_pyav(container, indices)\n",
    "\n",
    "            # Prepare video frames for model\n",
    "            inputs = image_processor(list(video_frames), return_tensors=\"pt\")\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            # Process the hidden states as needed\n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                print(f\"Layer {i} hidden state shape: {hidden_state.shape}\")\n",
    "\n",
    "            embedding = hidden_states[-1].detach().numpy().tolist()\n",
    "            embeddings[video_file] = embedding\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_file}: {e}\")\n",
    "\n",
    "\n",
    "# Path to save the NDJSON file (within the SageMaker environment)\n",
    "ndjson_file_path = './video_embeddings.ndjson'\n",
    "\n",
    "# Write the embeddings to the NDJSON file\n",
    "with open(ndjson_file_path, 'w') as ndjson_file:\n",
    "    for video_name, embedding in embeddings.items():\n",
    "        ndjson_file.write(json.dumps({video_name: embedding}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724dbde-4cc5-4aab-9083-b9c03eeed62e",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e82a72-4b81-4aa8-80d5-4e39ab6623e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the .ndjson file into a dictionary\n",
    "embeddings_dict = {}\n",
    "file_path = './video_embeddings.ndjson' \n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        for video_name, embedding in data.items():\n",
    "            embeddings_dict[video_name] = embedding\n",
    "\n",
    "# Prepare the embeddings for KNN\n",
    "# Convert embeddings to a numpy array for scikit-learn\n",
    "embeddings_list = list(embeddings_dict.values())\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "# Create and fit the KNN model\n",
    "knn = NearestNeighbors(n_neighbors=5, algorithm='auto')  # Adjust the number of neighbors as needed\n",
    "knn.fit(embeddings_array)\n",
    "\n",
    "# Querying the model\n",
    "# Replace 'your_video_name.mov' with an actual video name from  dataset\n",
    "video_name = 'your_video_name.mov'  # Update with an actual video name\n",
    "video_embedding = embeddings_dict[video_name]\n",
    "\n",
    "# Reshape and use KNN\n",
    "video_embedding_reshaped = np.array(video_embedding).reshape(1, -1)\n",
    "distances, indices = knn.kneighbors(video_embedding_reshaped)\n",
    "\n",
    "# Print Neighbors and Distances\n",
    "for i in range(len(indices[0])):\n",
    "    neighbor_video_name = list(embeddings_dict.keys())[indices[0][i]]\n",
    "    print(f\"Neighbor {i+1}: {neighbor_video_name}, Distance: {distances[0][i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
